{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk9x1ZgQp9DuBwG/4hRkOi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennywong524/di-cloze-project/blob/main/DI_cloze_test_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "Modify JSON Files: Make sure that each JSON record in your rawdata/booknlp_output_16k_jsonl files includes (or is updated with) a field like \"model\": \"o1\". This helps you later track which model produced which result.\n",
        "\n",
        "Example JSON record:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"book_id\": \"book123\",\n",
        "    \"passage\": \"The door opened, and [MASK], dressed and hatted, entered with a cup of tea.\",\n",
        "    \"model\": \"o1\"\n",
        "}\n",
        "```\n",
        "\n",
        "Sync Considerations\n",
        "If you are syncing data to Dropbox or another cloud service, verify that your modifications are updated on your remote storage before running the batch job."
      ],
      "metadata": {
        "id": "veZewMWjfF-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction script for o1\n",
        "\n",
        "Need to just modify the model in openai_predict_name_cloze_batch2.py script. The only change is to specify \"o1\" as the model in the API call (and update any prompt or endpoint specifics if needed)"
      ],
      "metadata": {
        "id": "91UMsXi2fSYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# OpenAI credentials\n",
        "# ------------------------------------------------------------------------\n",
        "API_ORG = os.environ.get(\"OPENAI_ORG\", \"\")\n",
        "API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "openai.organization = API_ORG\n",
        "openai.api_key = API_KEY\n",
        "\n",
        "def predict(passage):\n",
        "    \"\"\"\n",
        "    Given a passage with a [MASK] token, this function builds a prompt\n",
        "    and returns the model's prediction for the name that should fill in [MASK].\n",
        "    \"\"\"\n",
        "    prompt_text = f\"\"\"You have seen the following passage in your training data.\n",
        "What is the proper name that fills in the [MASK] token in it?\n",
        "This name is exactly one word long, and is a proper name (not a pronoun or any other word).\n",
        "You must make a guess, even if you are uncertain.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Stay gold, [MASK], stay gold.\"\n",
        "Output: <name>Ponyboy</name>\n",
        "\n",
        "Input: \"The door opened, and [MASK], dressed and hatted, entered with a cup of tea.\"\n",
        "Output: <name>Gerty</name>\n",
        "\n",
        "Input: {passage}\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "    # Note: Changing the model to \"o1\"\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"o1\",  # Using model \"o1\" instead of \"gpt-3.5-turbo\" or similar\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "        temperature=0.0\n",
        "    )\n",
        "    # Extract the output content from the API response\n",
        "    return completion[\"choices\"][0][\"message\"][\"content\"], completion\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Reads a JSONL file where each line is a JSON object with a passage.\n",
        "    Calls the predict() function for each passage and writes the output\n",
        "    (including any error messages) to an output JSONL file.\n",
        "    \"\"\"\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: openai_predict_name_cloze_batch2.py <input_jsonl> <output_jsonl>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    input_jsonl_path = sys.argv[1]\n",
        "    output_jsonl_path = sys.argv[2]\n",
        "\n",
        "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "         open(output_jsonl_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            record = json.loads(line)\n",
        "            passage = record.get(\"passage\", \"\")\n",
        "\n",
        "            try:\n",
        "                prediction, full_response = predict(passage)\n",
        "                record[\"prediction\"] = prediction\n",
        "                record[\"full_response\"] = full_response\n",
        "                record[\"model\"] = \"o1\"\n",
        "            except Exception as e:\n",
        "                record[\"prediction\"] = None\n",
        "                record[\"error\"] = str(e)\n",
        "\n",
        "            outfile.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "            time.sleep(0.1)  # Adjust sleep to handle rate limits if necessary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "vsP9gppzfR9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Execution Script\n",
        "Create a shell script (e.g., openai_predict_name_cloze_batch.sh) to process each JSONL file from our raw data folder. This script loops over the files and calls the above Python script."
      ],
      "metadata": {
        "id": "lCTVKX9KfuER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "# Usage:\n",
        "#   ./openai_predict_name_cloze_batch.sh rawdata/booknlp_output_16k_jsonl output/o1_results\n",
        "\n",
        "INPUT_FOLDER=$1\n",
        "OUTPUT_FOLDER=$2\n",
        "\n",
        "mkdir -p \"$OUTPUT_FOLDER\"\n",
        "\n",
        "for JSONL_FILE in \"$INPUT_FOLDER\"/*.jsonl; do\n",
        "    FILENAME=$(basename \"$JSONL_FILE\")\n",
        "    OUTPUT_FILE=\"$OUTPUT_FOLDER/$FILENAME\"\n",
        "\n",
        "    echo \"Processing $JSONL_FILE -> $OUTPUT_FILE\"\n",
        "    python3 openai_predict_name_cloze_batch2.py \"$JSONL_FILE\" \"$OUTPUT_FILE\"\n",
        "done"
      ],
      "metadata": {
        "id": "GkECCZVmfsYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Processing the Results\n",
        "\n",
        "Our current pipeline uses a script (e.g., process_openai_batch3.py) to extract and combine the output batches. You can reuse that script with the new results directory.\n",
        "\n",
        "If any minor modifications are needed (for example, to check for the field \"model\": \"o1\"), update your post-processing script accordingly. The general steps are:\n",
        "\n",
        "Merge JSONL Files: Concatenate all JSONL output files into one master file.\n",
        "\n",
        "Extract Relevant Data: Parse out fields such as book_id, passage, prediction, and optionally any confidence metrics.\n",
        "\n",
        "Compute Metrics: If desired, compare predictions against known answers (if available) to compute accuracy.\n"
      ],
      "metadata": {
        "id": "UZdOKcfJgDBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow\n",
        "\n",
        "1. Prepare data: Update the JSONL files with \"model\": \"o1\"\n",
        "2. Run Predictions:\n",
        "\n",
        "\n",
        "```\n",
        "./openai_predict_name_cloze_batch.sh rawdata/booknlp_output_16k_jsonl output/o1_results\n",
        "```\n",
        "3. Post-Process Results:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "./process_openai_batch.sh output/o1_results combined_results/o1_all.jsonl\n",
        "```\n",
        "\n",
        "4.  Use analysis tools (Python, Jupyter notebooks, etc.) to evaluate the performance of model “o1” on the name-cloze task.\n"
      ],
      "metadata": {
        "id": "ZBDHQpA7gcT7"
      }
    }
  ]
}